{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # 数据分析\n",
    "import numpy as np  # 科学计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train.columns                         # 读取数据首行信息（表头信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()     # 给出了dataframe的数据结构，891个样本，每个表头ID的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(5)   # 打印前面五条信息，查看表格结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体上对数据进行描述，找到可能与survived有关的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.describe()     #我们最终想要得到的结果是什么因数会影响到survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初步结论：\n",
    "#### 1.因为survived的值为{0,1}所以mean=0.38就是891个样本获救的比例\n",
    "#### 2.仓位数的平均值mean=2.3>2 ，说明2，3等仓位多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进行数据可视化处理，分析各个属性与survived之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline      #与plot()等价效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.Survived.value_counts().plot(kind = \"bar\")  \n",
    "a = data_train.Survived.value_counts()\n",
    "print(a)\n",
    "print(type(data_train.Survived.value_counts()))    \n",
    "# a = pd.Series(data_train.Survived.value_counts(),index=[\"a\",\"b\",\"c\"])\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### kind参数：’line’, ‘bar’, ‘barh’, ‘kde’ 等\n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))   # 设置整个图形的size  \n",
    "fig.set(alpha = 0.2)               # 设置图表颜色alpha参数,\t图表的填充不透明(0-1)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0))   # 生成两行三列的图标，此时位于第一行第一列\n",
    "#### 获救的整体情况进行统计，生成柱状图。\n",
    "\"\"\"对无X，Y之间没有函数关系（显性解析式表达）的数据之间采用此种plot()\"\"\"\n",
    "\"\"\"此种图表隐含的意思是：Survived的状态与count之间的关系\"\"\"\n",
    "data_train.Survived.value_counts().plot(kind = \"bar\")  \t# kind参数可以是’line’, ‘bar’, ‘barh’, ‘kde’\n",
    "print(data_train.Survived.value_counts().shape)    # 打印data_train.Survived.value_counts()的结构shape\n",
    "print(data_train.Survived.value_counts())          # 打印其值\n",
    "print(type(data_train.Survived.value_counts()))    # 打印数据类型 series的数据结构应该是一个一维的ndarry结构。   \n",
    "print(data_train.Survived.value_counts()[1])       # 说明前面的{0，1}为其索引值\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Survived=1\")\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "#### 对仓位进行统计\n",
    "data_train.Pclass.value_counts().plot(kind = \"bar\")  \n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"distribution by class\")\n",
    "\n",
    "plt.subplot2grid((2,3),(0,2))\n",
    "#### 对获救者的年龄分布进行分析（获救状态为横坐标，年龄分布为纵坐标）\n",
    "plt.scatter(data_train.Survived,data_train.Age)\n",
    "plt.ylabel(\"age\")\n",
    "plt.title(\"distribution by age\")\n",
    "plt.grid(b=True,which=\"major\",axis=\"y\")   # 显示背景网格\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0),colspan=2)\n",
    "#### 复合属性与survived之间的关系，不同仓位的年龄密度分布\n",
    "\"\"\"获取符合属性的方法，data.A[B]——在B条件下的A\"\"\"\n",
    "\"\"\"对无X，Y之间没有函数关系（显性解析式表达）的数据之间采用此种plot()\"\"\"\n",
    "data_train.Age[data_train.Pclass==1].plot(kind=\"kde\")    # 统计密度曲线（kernel density estimate）\n",
    "data_train.Age[data_train.Pclass==2].plot(kind=\"kde\") \n",
    "data_train.Age[data_train.Pclass==3].plot(kind=\"kde\") \n",
    "plt.xlabel(\"age\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.title(\"distribution by age&class\")\n",
    "plt.legend((\"Pclass == 1\",\"Pclass == 2\",\"Pclass ==3\"),loc=\"best\") # 图例显示，与plot的曲线顺序对应，loc系统默认在最佳位置。\n",
    "\n",
    "plt.subplot2grid((2,3),(1,2))        # 注意此时的（1，2）\n",
    "#### 对登陆口岸分布进行分析（获救状态为横坐标，年龄分布为纵坐标）\n",
    "data_train.Embarked.value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"count by Embarked\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由上面图示，可以更加直观的看到各属性与survived之间的联系\n",
    "#### 1.3等仓的人数最多，2等最少，\n",
    "#### 2.获救与未获救的年龄分布大致相同，最大年龄的为获救\n",
    "#### 3.年龄密度分布曲线，平均年龄与仓位数对应\n",
    "#### 4.S口岸登陆的最多，Q口岸登陆的最少\n",
    "\n",
    "#### 根据上面得到的信息结论，进一步的详细分析分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"分析仓位与获救的关系\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "# 统计仓位与survived的数量\n",
    "Survived_0 = data_train.Pclass[data_train.Survived==0].value_counts()\n",
    "Survived_1 = data_train.Pclass[data_train.Survived==1].value_counts()\n",
    "print(Survived_0.shape)   # （3，）一维数组中标量的个数为3，series数据结构.{1,2,3}作为索引值\n",
    "print(Survived_0)  \n",
    "df = pd.DataFrame({\"Survived_0\":Survived_0, \"Survived_1\":Survived_1})    # 添加columns信息\n",
    "print(df)\n",
    "# df = pd.DataFrame([Survived_0,Survived_1],[\"Survived_0\",\"Survived_1\"])   # 会对图表的生成方式造成影响\n",
    "df.plot(kind=\"bar\",stacked=True)\n",
    "plt.title(\"Survived by Pcalss\")\n",
    "plt.xlabel(\"class\")\n",
    "plt.ylabel(\"number\")\n",
    "# plt.legend((\"the first class\",\"the second class\",\"the third class\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论：可以看到1等仓获救比例远高于3等仓，说明仓位是影响是否获救的一个重要影响因素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 另外一种展示方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "\"\"\"DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)\"\"\"\n",
    "# 可以对data传入index和columns\n",
    "\n",
    "# 统计仓位与survived的数量\n",
    "Survived_0 = data_train.Pclass[data_train.Survived==0].value_counts()\n",
    "Survived_1 = data_train.Pclass[data_train.Survived==1].value_counts()\n",
    "print(Survived_0)\n",
    "# df = pd.DataFrame({\"Survived_0\":Survived_0, \"Survived_1\":Survived_1})  \n",
    "df = pd.DataFrame([Survived_0,Survived_1],[\"Survived_0\",\"Survived_1\"])   # 对series数据结构添加index,原来的索引值变为columns值 \n",
    "print(df)\n",
    "# df = pd.DataFrame([Survived_0,Survived_1],[\"Survived_0\",\"Survived_1\"]).transpose()   # 可以通过transpose（）转置将column与index对调。\n",
    "df.plot(kind=\"bar\",stacked=True)\n",
    "plt.title(\"Survived by Pcalss\")\n",
    "# plt.xlabel(\"class\")\n",
    "# plt.ylabel(\"number\")\n",
    "# plt.legend((\"the first class\",\"the second class\",\"the third class\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分析survived与登陆口岸之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "Survived_0 = data_train.Embarked[data_train.Survived==0].value_counts()\n",
    "Survived_1 = data_train.Embarked[data_train.Survived==1].value_counts()\n",
    "df = pd.DataFrame({\"Survived_0\":Survived_0, \"Survived_1\":Survived_1})\n",
    "df.plot(kind=\"bar\",stacked=True)\n",
    "plt.title(\"Survived by Pmbarked\")\n",
    "plt.xlabel(\"Embarked\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各个登陆口岸之间似乎没有太大的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 分析性别与获救之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "### 以性别区分是否获救\n",
    "Survived_0 = data_train.Sex[data_train.Survived==0].value_counts()\n",
    "Survived_1 = data_train.Sex[data_train.Survived==1].value_counts()\n",
    "df = pd.DataFrame({\"Survived_0\":Survived_0, \"Survived_1\":Survived_1})\n",
    "df.plot(kind=\"bar\",stacked=True)\n",
    "plt.title(\"Survived by Sexual\")\n",
    "plt.xlabel(\"Sexual\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  以是否获救来统计相应的性别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "Survived_m = data_train.Survived[data_train.Sex==\"male\"].value_counts()\n",
    "Survived_f = data_train.Survived[data_train.Sex==\"female\"].value_counts()\n",
    "df = pd.DataFrame({\"Survived_f\":Survived_f, \"Survived_m\":Survived_m})\n",
    "df.plot(kind=\"bar\",stacked=True)\n",
    "plt.title(\"Survived by Sexual\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 详细的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "fig.set(alpha=0.2)\n",
    "\n",
    "plt.subplot2grid((1,4),(0,0))    #此种生成子图的方式与subplot(1,4,1) 的标记方式不同。元组表示（row,column）标记图示位置\n",
    "data_train.Survived[data_train.Sex==\"female\"][data_train.Pclass != 3].value_counts().plot(kind= \"bar\",label= \"female highcalss\",color=\"red\")\n",
    "plt.title(\"female&Pclass==1|2\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.subplot2grid((1,4),(0,1))\n",
    "data_train.Survived[data_train.Sex==\"female\"][data_train.Pclass == 3].value_counts().plot(kind= \"bar\",label= \"female lowcalss\",color=\"g\")\n",
    "plt.title(\"female&Pclass==3\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.subplot2grid((1,4),(0,2))\n",
    "data_train.Survived[data_train.Sex==\"male\"][data_train.Pclass != 3].value_counts().plot(kind= \"bar\",label= \"male highcalss\",color=\"b\")\n",
    "plt.title(\"female&Pclass==1|2\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.subplot2grid((1,4),(0,3))\n",
    "data_train.Survived[data_train.Sex==\"male\"][data_train.Pclass == 3].value_counts().plot(kind= \"bar\",label= \"male lowcalss\",color=\"#FF00AA\")\n",
    "plt.title(\"female&Pclass==3\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论：女性不同仓位对survived的影响较大；而不同仓位对男性的影响相对较小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看堂兄数目与获救之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data_train.groupby([\"SibSp\",\"Survived\"])    # 按照\"SibSp\",\"Survived\"两个维度的属性对数据集进行聚类\n",
    "# print(g.count())\n",
    "df = pd.DataFrame(g.count()[\"PassengerId\"])     # 对聚类结果，按PassengerId进行计数，重点在于计数（对PassengerId的个数进行累计）\n",
    "# df = pd.DataFrame(g.count()[\"Pclass\"])        # 得到同样的结果，只是属性换成“Pclass”\n",
    "sum = np.sum(df[\"PassengerId\"])\n",
    "# print(sum)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  结论：SibSp的变化对于survived的影响似乎很小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看Parch与获救之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data_train.groupby([\"Parch\",\"Survived\"])\n",
    "# print(g.count())\n",
    "df = pd.DataFrame(g.count()[\"PassengerId\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论：当Parch不为0时，获救的相对概率有所增加。说明由子女或是父母是能增加获救率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cabin的值只有204个，先整体看一下这个属性的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  cabin作为类目属性的信息，太过于分散（绝大部分只是出现一次），加入特征未必会有效。\n",
    "#  那我们一起看看这个值的有无，对于survival的分布状况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_cabin = data_train.Survived[pd.notnull(data_train.Cabin)].value_counts()    # cabin非空的统计\n",
    "# print(Survived_cabin.shape)\n",
    "# print(Survived_cabin)\n",
    "# print(type(Survived_cabin))\n",
    "Survived_nocabin = data_train.Survived[pd.isnull(data_train.Cabin)].value_counts()   # cabin空值统计\n",
    "df=pd.DataFrame({'Survived_cabin':Survived_cabin, 'Survived_nocabin':Survived_nocabin})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Survived by Cabin_0/1\")\n",
    "plt.xlabel(\"Survived\") \n",
    "plt.ylabel(u\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4 color= black>**结论：**<font><br>\n",
    "   <font size=3 color=\"red\">1.获救群体中，有cabin记录的比例要高一些，说明cabin属性是会影响获救。<font> <br>\n",
    " \n",
    "   <font size=3 color=\"red\">2.cabin是一个类目型的属性值，具体数据太过于分散，而且并没有实际意义。故而将cabin属性分为YES\\NO,或是（1\\0）<font>\n",
    "\n",
    "\n",
    "<font color= black>**再说Age：**<font><br>\n",
    "\n",
    "<font color=red>通常遇到缺值的情况，我们会有几种常见的处理方式<font><br>\n",
    "\n",
    "1. <font color=red>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了<font><br>\n",
    "2. <font color=red>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中<font><br>\n",
    "3. <font color=red>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。<font><br>\n",
    "4. <font color=red>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。<font><br>\n",
    "<font color=red>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(虽然说没有特别多的背景可供我们拟合，这不一定是一个多么好的选择)<font><br>\n",
    "\n",
    "<font color=red>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]  # 获取列表中的各属性值信息,将Age作为预测值，其余的作为属性值用来预估Age.\n",
    "    # print(age_df)\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]   # 获取已知年龄的值作为 目标值\n",
    "    print(type(y))\n",
    "    print(y.shape)\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]  # 目标值与样本值分离\n",
    "\n",
    "\n",
    "    # fit到RandomForestRegressor之中，进行模型训练\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)        # 得到拟合结果\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges   # 将预测值按顺序赋给空值\n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"   # 对Cabin属性做标签化处理\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "\n",
    "data_train,rfr = set_missing_ages(data_train)   # 接受 df 和 rfr 返回值\n",
    "data_train = set_Cabin_type(data_train)       # 接受df 返回值\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3>** One-hot编码 **<font><bar>\n",
    "<font color=black>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先<u>**对类目型的特征因子化/one-hot编码**</u>。 <font><br>\n",
    "<font color=red>什么叫做因子化/one-hot编码？举个例子：<font><br>\n",
    "\n",
    "<font color=black>以Embarked为例，原本一个属性维度，因为其取值可以是[‘S’,’C’,’Q‘]，而将其平展开为’Embarked_C’,’Embarked_S’, ‘Embarked_Q’三个属性<font><br>\n",
    "\n",
    "* <font color=black>原本Embarked取值为S的，在此处的”Embarked_S”下取值为1，在’Embarked_C’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=black>原本Embarked取值为C的，在此处的”Embarked_C”下取值为1，在’Embarked_S’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=black>原本Embarked取值为Q的，在此处的”Embarked_Q”下取值为1，在’Embarked_C’, ‘Embarked_S’下取值为0<font><br>\n",
    "\n",
    "<font color=black>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为逻辑回归建模时，需要输入的特征都是数值型特征\n",
    "# 我们先对类目型的特征离散/因子化\n",
    "# 以Cabin为例，原本一个属性维度，因为其取值可以是['yes','no']，而将其平展开为'Cabin_yes','Cabin_no'两个属性\n",
    "# 原本Cabin取值为yes的，在此处的'Cabin_yes'下取值为1，在'Cabin_no'下取值为0\n",
    "# 原本Cabin取值为no的，在此处的'Cabin_yes'下取值为0，在'Cabin_no'下取值为1\n",
    "# 我们使用pandas的get_dummies来完成这个工作，并拼接在原来的data_train之上，如下所示\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** drop（）函数**\n",
    "<font>drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') <font><bar>\n",
    "<font>Drop specified labels from rows or columns.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= red size=3>**标准化/归一化**<font><bar>\n",
    "<font color=black>我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！<br>\n",
    "    所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来我们要接着做一些数据预处理的工作，比如scaling，将一些变化幅度较大的特征化到[-1,1]之内\n",
    "# 这样可以加速logistic regression的收敛\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()          # 标准化，处理后的数据服从标准正态分布。\n",
    "a = np.c_[df['Age'],df['Age']]\n",
    "# a = np.stack((df['Age'],df['Age']),axis=1)     # 利用stack（）对两列进行堆叠得到新的数组\n",
    "age_scale_param = scaler.fit(a)                  # 传入待训练参数进行拟合\n",
    "df['Age_scaled'] = scaler.fit_transform(a, age_scale_param)[:,0]  # 利用拟合结果对现有数据进行规范化,返回的结果columns为'Age_scaled'\n",
    "df['Age_scaled'] = scaler.fit_transform(a, age_scale_param) \n",
    "b = np.c_[df['Fare'],df['Fare']]\n",
    "fare_scale_param = scaler.fit(b)\n",
    "df['Fare_scaled'] = scaler.fit_transform(b, fare_scale_param)[:,0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取所需要的feature值，进行建模\n",
    "#### <font size=4 color=red>1、LogisticRegression </font><bar>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0:1]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "print(\"accuracy:\",clf.score(X,y))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试集做一样的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0          # 对Fare属性的空值做补0处理\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]    # 提取测试集数据相应的属性值\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()             # 将Age为空值的样本转化为矩阵\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]                                               # 为了预测空值的Age值，将除Age以外的特征抽取出来作为测试数据\n",
    "predictedAges = rfr.predict(X)                                    #  利用在训练集得到的模型参数，对该测试数据进行预测得到Age值。\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges   # 将测试数据Age属性为空值的利用预测值补上\n",
    "\n",
    "data_test = set_Cabin_type(data_test)                             # 调用函数对Cabin值进行处理（YES/NO）\n",
    "# 对离散值的属性进行one-hot编码\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "# 对进行scaled后的数据进行拼接，及去掉多余的属性列\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "# Age与Fare值跨度较大，进行规范化处理\n",
    "df_test['Age_scaled'] = scaler.fit_transform(np.c_[df_test['Age'],df_test['Age']], age_scale_param)[:,0]\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(np.c_[df_test['Fare'],df_test['Fare']], fare_scale_param)[:,0]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red size=4>得到预测结果，写入本地文件<font><bar>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')   #利用filter进行feature筛选（regex）\n",
    "predictions = clf.predict(test)     # 利用训练集上训练好的模型进行预测\n",
    "\n",
    "# 选取所需要的信息写入本地文件\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"logistic_regression_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red size=4>以上就是我们得到的一个模型概况baseline，接下来对其进行优化处理</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>接下来，我们就该看看如何优化baseline系统了<br>\n",
    "我们还有些特征可以再挖掘挖掘<br><br>\n",
    "\n",
    "1. 比如说Name和Ticket两个属性被我们完整舍弃了(好吧，其实是一开始我们对于这种，每一条记录都是一个完全不同的值的属性，并没有很直接的处理方式)<br>\n",
    "2. 比如说，我们想想，年龄的拟合本身也未必是一件非常靠谱的事情<br>\n",
    "3. 另外，以我们的日常经验，小盆友和老人可能得到的照顾会多一些，这样看的话，年龄作为一个连续值，给一个固定的系数，似乎体现不出两头受照顾的实际情况，所以，说不定我们把年龄离散化，按区段分作类别属性会更合适一些<br>\n",
    "\n",
    "那怎么样才知道，哪些地方可以优化，哪些优化的方法是promising的呢？<br>\n",
    "是的<br><br>\n",
    "\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br><br>\n",
    "\n",
    "重要的事情说3编！！！<br>\n",
    "因为test.csv里面并没有Survived这个字段(好吧，这是废话，这明明就是我们要预测的结果)，我们无法在这份数据上评定我们算法在该场景下的效果。。。<br>\n",
    "我们通常情况下，这么做cross validation：把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。<br>\n",
    "我们可以用scikit-learn的cross_validation来完成这个工作</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(train_df.columns)[1:]    # 所有参与训练的特征feature \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = list(clf.coef_)      # 各个特征相应的系数\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到所有的特征和相应的系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\":list(train_df.columns)[1:], \"coef\":list(clf.coef_.T)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "    上面的系数和最后的结果是一个正相关的关系<br>\n",
    "我们先看看那些权重绝对值非常大的feature，在我们的模型上：<br>\n",
    "\n",
    "* Sex属性，如果是female会极大提高最后获救的概率，而male会很大程度拉低这个概率。\n",
    "* Pclass属性，1等舱乘客最后获救的概率会上升，而乘客等级为3会极大地拉低这个概率。\n",
    "* 有Cabin值会很大程度拉升最后获救概率<br>(这里似乎能看到了一点端倪，事实上从最上面的有无Cabin记录的Survived分布图上看出，即使有Cabin记录的乘客也有一部分遇难了，估计这个属性上我们挖掘还不够)\n",
    "* Age是一个负相关，意味着在我们的模型里，年龄越小，越有获救的优先权(还得回原数据看看这个是否合理）\n",
    "* 有一个登船港口S会很大程度拉低获救的概率，另外俩港口压根就没啥作用<br>(这个实际上非常奇怪，因为我们从之前的统计图上并没有看到S港口的获救率非常低，所以也许可以考虑把登船港口这个feature去掉试试)。\n",
    "* 船票Fare有小幅度的正相关(并不意味着这个feature作用不大，有可能是我们细化的程度还不够，举个例子，说不定我们得对它离散化，再分至各个乘客等级上？)\n",
    "\n",
    "噢啦，观察完了，我们现在有一些想法了，但是怎么样才知道，哪些优化的方法是promising的呢？<br>\n",
    "\n",
    "恩，要靠交叉验证<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# 简单看看打分情况\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "all_data = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "X = all_data.as_matrix()[:,1:]\n",
    "y = all_data.as_matrix()[:,0]\n",
    "print(cross_val_score(clf, X, y, cv=5))     # 进行交叉验证评分，测试训练集所得模型参数的稳定性。\n",
    "\n",
    "\n",
    "# 分割数据\n",
    "split_train, split_cv = train_test_split(df, test_size=0.3, random_state=0)    # 对训练集数据进行分割\n",
    "train_df = split_train.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "# 生成模型\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(train_df.as_matrix()[:,1:], train_df.as_matrix()[:,0])\n",
    "\n",
    "# 对cross validation数据进行预测\n",
    "cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')  # 测试集数据特性\n",
    "predictions = clf.predict(cv_df.as_matrix()[:,1:])\n",
    "print(cv_df.shape)   \n",
    "print(clf.score(cv_df.as_matrix()[:,1:], cv_df.as_matrix()[:,0]))    #分类器评分\n",
    "badcase = cv_df[predictions != cv_df.as_matrix()[:,0]]   # 找出预测值与真实值不符的样本\n",
    "print(badcase.shape)\n",
    "badcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测错误的case看原始dataframe数据\n",
    "origin_data_train = pd.read_csv(\"train.csv\")\n",
    "bad_cases_origin = origin_data_train.loc[origin_data_train['PassengerId'].isin(split_cv[predictions != cv_df.as_matrix()[:,0]]['PassengerId'].values)]\n",
    "print(bad_cases_origin.shape)          # 前面得到的是经过数据处理后预测值与真实值不符的样本，现在我们找这些样本的原始值\n",
    "bad_cases_origin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>对比bad case，我们仔细看看我们预测错的样本，到底是哪些特征有问题，咱们处理得还不够细？<br>\n",
    "\n",
    "我们随便列一些可能可以做的优化操作：<br>\n",
    "\n",
    "* Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。\n",
    "* Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。\n",
    "* Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。\n",
    "* Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。\n",
    "* 单加一个Child字段，Age<=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)\n",
    "* 如果名字里面有『Mrs』，而Parch>1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0\n",
    "* 登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)\n",
    "* 把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)\n",
    "* Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。\n",
    "\n",
    "大家接着往下挖掘，可能还可以想到更多可以细挖的部分。我这里先列这些了，然后我们可以使用手头上的”train_df”和”cv_df”开始试验这些feature engineering的tricks是否有效了。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train[data_train['Name'].str.contains(\"Major\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train['Sex_Pclass'] = data_train.Sex + \"_\" + data_train.Pclass.map(str)    # 性别与仓位联合组合作为新的属性\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_train['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "a = np.c_[df['Age'],df['Age']]\n",
    "age_scale_param = scaler.fit(a)\n",
    "df['Age_scaled'] = scaler.fit_transform(a, age_scale_param)[:,0]\n",
    "b = np.c_[df['Fare'],df['Fare']]\n",
    "fare_scale_param = scaler.fit(b)\n",
    "df['Fare_scaled'] = scaler.fit_transform(b, fare_scale_param)[:,0]\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "clf\n",
    "print('accuracy = ',clf.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "data_test['Sex_Pclass'] = data_test.Sex + \"_\" + data_test.Pclass.map(str)\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_test['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "\n",
    "df_test['Age_scaled'] = scaler.fit_transform(np.c_[df_test['Age'],df_test['Age']], age_scale_param)[:,0]\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(np.c_[df_test['Fare'],df_test['Fare']], fare_scale_param)[:,0]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  <font color=red>一般做到后期，咱们要进行模型优化的方法就是模型融合啦<br>\n",
    "先解释解释啥叫模型融合哈，我们还是举几个例子直观理解一下好了。<br><br>\n",
    "\n",
    "大家都看过知识问答的综艺节目中，求助现场观众时候，让观众投票，最高的答案作为自己的答案的形式吧，每个人都有一个判定结果，最后我们相信答案在大多数人手里。<br>\n",
    "\n",
    "再通俗一点举个例子。你和你班某数学大神关系好，每次作业都『模仿』他的，于是绝大多数情况下，他做对了，你也对了。突然某一天大神脑子犯糊涂，手一抖，写错了一个数，于是…恩，你也只能跟着错了。 <br>\n",
    "我们再来看看另外一个场景，你和你班5个数学大神关系都很好，每次都把他们作业拿过来，对比一下，再『自己做』，那你想想，如果哪天某大神犯糊涂了，写错了，but另外四个写对了啊，那你肯定相信另外4人的是正确答案吧？<br>\n",
    "\n",
    "最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。<br>\n",
    "\n",
    "bingo，问题就这么完美的解决了。<br>\n",
    "\n",
    "模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。<br>\n",
    "\n",
    "话说回来，回到我们现在的问题。你看，我们现在只讲了logistic regression，如果我们还想用这个融合思想去提高我们的结果，我们该怎么做呢？<br>\n",
    "\n",
    "既然这个时候模型没得选，那咱们就在数据上动动手脚咯。大家想想，如果模型出现过拟合现在，一定是在我们的训练上出现拟合过度造成的对吧。<br>\n",
    "\n",
    "那我们干脆就不要用全部的训练集，每次取训练集的一个subset，做训练，这样，我们虽然用的是同一个机器学习算法，但是得到的模型却是不一样的；同时，因为我们没有任何一份子数据集是全的，因此即使出现过拟合，也是在子训练集上出现过拟合，而不是全体数据上，这样做一个融合，可能对最后的结果有一定的帮助。对，这就是常用的Bagging。<br>\n",
    "\n",
    "我们用scikit-learn里面的Bagging来完成上面的思路，过程非常简单。代码如下：<br><br><font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到BaggingRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "bagging_clf = BaggingRegressor(clf, n_estimators=10, max_samples=0.8, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1)\n",
    "bagging_clf.fit(X, y)\n",
    "\n",
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "predictions = bagging_clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
