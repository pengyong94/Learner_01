{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T03:10:03.201443Z",
     "start_time": "2019-03-08T03:10:03.198125Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对句子进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:09:49.398648Z",
     "start_time": "2019-04-22T15:09:49.395173Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_sentences = [\"The research subjects are the 29 English teachers of First Office of College English Department of HIT and some of their students.\",\"Microsoft Office 2013, free and safe download. Microsoft Office 2013 latest version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:09:49.573924Z",
     "start_time": "2019-04-22T15:09:49.569460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'research', 'subjects', 'are', 'the', '29', 'English', 'teachers', 'of', 'First', 'Office', 'of', 'College', 'English', 'Department', 'of', 'HIT', 'and', 'some', 'of', 'their', 'students.'], ['Microsoft', 'Office', '2013,', 'free', 'and', 'safe', 'download.', 'Microsoft', 'Office', '2013', 'latest', 'version']]\n"
     ]
    }
   ],
   "source": [
    "sentence = [s.split() for s in raw_sentences]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim构建word2vec的参数\n",
    "gensim中，word2vec 相关的API都在包gensim.models.word2vec中。和算法有关的参数都在类gensim.models.word2vec.Word2Vec中。算法需要注意的参数有：\n",
    "\n",
    "　　　　1) sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。后面我们会有从文件读出的例子。\n",
    "\n",
    "　　　　2) size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。\n",
    "\n",
    "　　　　3) window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为c，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。\n",
    "\n",
    "　　　　4) sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。\n",
    "\n",
    "　　　　5) hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。\n",
    "\n",
    "　　　　6) negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。\n",
    "\n",
    "　　　　7) cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xw,默认值也是1,不推荐修改默认值。\n",
    "\n",
    "　　　　8) min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。\n",
    "\n",
    "　　　　9) iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。\n",
    "\n",
    "　　　　10) alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为η，默认是0.025。\n",
    "\n",
    "　　　　11) min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:09:49.933646Z",
     "start_time": "2019-04-22T15:09:49.885532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-22 23:09:49,886 : INFO : collecting all words and their counts\n",
      "2019-04-22 23:09:49,887 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-22 23:09:49,888 : INFO : collected 26 word types from a corpus of 34 raw words and 2 sentences\n",
      "2019-04-22 23:09:49,888 : INFO : Loading a fresh vocabulary\n",
      "2019-04-22 23:09:49,889 : INFO : min_count=1 retains 26 unique words (100% of original 26, drops 0)\n",
      "2019-04-22 23:09:49,889 : INFO : min_count=1 leaves 34 word corpus (100% of original 34, drops 0)\n",
      "2019-04-22 23:09:49,890 : INFO : deleting the raw counts dictionary of 26 items\n",
      "2019-04-22 23:09:49,891 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2019-04-22 23:09:49,892 : INFO : downsampling leaves estimated 6 word corpus (18.3% of prior 34)\n",
      "2019-04-22 23:09:49,893 : INFO : estimated required memory for 26 words and 100 dimensions: 33800 bytes\n",
      "2019-04-22 23:09:49,894 : INFO : resetting layer weights\n",
      "2019-04-22 23:09:49,897 : INFO : training model with 3 workers on 26 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-22 23:09:49,901 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:09:49,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:09:49,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:09:49,903 : INFO : EPOCH - 1 : training on 34 raw words (6 effective words) took 0.0s, 3009 effective words/s\n",
      "2019-04-22 23:09:49,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:09:49,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:09:49,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:09:49,909 : INFO : EPOCH - 2 : training on 34 raw words (6 effective words) took 0.0s, 1939 effective words/s\n",
      "2019-04-22 23:09:49,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:09:49,914 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:09:49,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:09:49,915 : INFO : EPOCH - 3 : training on 34 raw words (10 effective words) took 0.0s, 4247 effective words/s\n",
      "2019-04-22 23:09:49,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:09:49,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:09:49,919 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:09:49,920 : INFO : EPOCH - 4 : training on 34 raw words (6 effective words) took 0.0s, 3554 effective words/s\n",
      "2019-04-22 23:09:49,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:09:49,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:09:49,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:09:49,929 : INFO : EPOCH - 5 : training on 34 raw words (6 effective words) took 0.0s, 1257 effective words/s\n",
      "2019-04-22 23:09:49,930 : INFO : training on a 170 raw words (34 effective words) took 0.0s, 1082 effective words/s\n",
      "2019-04-22 23:09:49,930 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# 构建word2vec 模型\n",
    "model = word2vec.Word2Vec(sentence,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T03:20:04.437825Z",
     "start_time": "2019-03-08T03:20:04.266750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.06650513"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"Microsoft\",\"Office\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关性比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:05:00.607327Z",
     "start_time": "2019-04-22T15:05:00.360319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-22 23:05:00,364 : INFO : collecting all words and their counts\n",
      "2019-04-22 23:05:00,364 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-22 23:05:00,365 : INFO : collected 15 word types from a corpus of 16 raw words and 2 sentences\n",
      "2019-04-22 23:05:00,366 : INFO : Loading a fresh vocabulary\n",
      "2019-04-22 23:05:00,367 : INFO : min_count=1 retains 15 unique words (100% of original 15, drops 0)\n",
      "2019-04-22 23:05:00,367 : INFO : min_count=1 leaves 16 word corpus (100% of original 16, drops 0)\n",
      "2019-04-22 23:05:00,368 : INFO : deleting the raw counts dictionary of 15 items\n",
      "2019-04-22 23:05:00,369 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2019-04-22 23:05:00,370 : INFO : downsampling leaves estimated 2 word corpus (13.7% of prior 16)\n",
      "2019-04-22 23:05:00,370 : INFO : estimated required memory for 15 words and 100 dimensions: 19500 bytes\n",
      "2019-04-22 23:05:00,371 : INFO : resetting layer weights\n",
      "2019-04-22 23:05:00,409 : INFO : training model with 3 workers on 15 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-22 23:05:00,479 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:05:00,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:05:00,507 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:05:00,508 : INFO : EPOCH - 1 : training on 16 raw words (2 effective words) took 0.0s, 68 effective words/s\n",
      "2019-04-22 23:05:00,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:05:00,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:05:00,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:05:00,512 : INFO : EPOCH - 2 : training on 16 raw words (3 effective words) took 0.0s, 1452 effective words/s\n",
      "2019-04-22 23:05:00,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:05:00,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:05:00,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:05:00,523 : INFO : EPOCH - 3 : training on 16 raw words (1 effective words) took 0.0s, 156 effective words/s\n",
      "2019-04-22 23:05:00,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:05:00,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:05:00,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:05:00,529 : INFO : EPOCH - 4 : training on 16 raw words (2 effective words) took 0.0s, 842 effective words/s\n",
      "2019-04-22 23:05:00,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-22 23:05:00,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-22 23:05:00,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-22 23:05:00,537 : INFO : EPOCH - 5 : training on 16 raw words (2 effective words) took 0.0s, 591 effective words/s\n",
      "2019-04-22 23:05:00,538 : INFO : training on a 80 raw words (10 effective words) took 0.1s, 78 effective words/s\n",
      "2019-04-22 23:05:00,538 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "D:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.003932551024755548"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 引入数据集\n",
    "raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\n",
    "\n",
    "# 切分词汇\n",
    "sentences= [s.split() for s in raw_sentences]\n",
    "\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# 进行相关性比较\n",
    "model.similarity('dogs','you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写入文本，并进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:43:22.495147Z",
     "start_time": "2019-04-22T16:43:22.462907Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "with open(\"data.txt\") as f:\n",
    "    document = f.read()\n",
    "    \n",
    "    #document_decode = document.decode('GBK')\n",
    "    \n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)  //如果打印结果，则分词效果消失，后面的result无法显示\n",
    "    result = ' '.join(document_cut)\n",
    "    result = result.encode('utf-8')\n",
    "    with open('data_01.txt', 'wb') as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拿到了分词后的文件，在一般的NLP处理中，会需要去停用词。由于word2vec的算法依赖于上下文，而上下文有可能就是停词。因此对于word2vec，我们可以不用去停词。\n",
    "\n",
    "　　　　现在我们可以直接读分词后的文件到内存。这里使用了word2vec提供的LineSentence类来读文件，然后套用word2vec的模型。这里只是一个示例，因此省去了调参的步骤，实际使用的时候，你可能需要对我们上面提到一些参数进行调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:43:23.448298Z",
     "start_time": "2019-04-22T16:43:23.378834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:43:23,381 : INFO : collecting all words and their counts\n",
      "2019-04-23 00:43:23,383 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-23 00:43:23,384 : INFO : collected 247 word types from a corpus of 467 raw words and 4 sentences\n",
      "2019-04-23 00:43:23,385 : INFO : Loading a fresh vocabulary\n",
      "2019-04-23 00:43:23,386 : INFO : min_count=1 retains 247 unique words (100% of original 247, drops 0)\n",
      "2019-04-23 00:43:23,387 : INFO : min_count=1 leaves 467 word corpus (100% of original 467, drops 0)\n",
      "2019-04-23 00:43:23,388 : INFO : deleting the raw counts dictionary of 247 items\n",
      "2019-04-23 00:43:23,389 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2019-04-23 00:43:23,390 : INFO : downsampling leaves estimated 297 word corpus (63.8% of prior 467)\n",
      "2019-04-23 00:43:23,391 : INFO : constructing a huffman tree from 247 words\n",
      "2019-04-23 00:43:23,398 : INFO : built huffman tree with maximum node depth 9\n",
      "2019-04-23 00:43:23,399 : INFO : estimated required memory for 247 words and 100 dimensions: 469300 bytes\n",
      "2019-04-23 00:43:23,400 : INFO : resetting layer weights\n",
      "2019-04-23 00:43:23,407 : INFO : training model with 3 workers on 247 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2019-04-23 00:43:23,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:43:23,413 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:43:23,414 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:43:23,414 : INFO : EPOCH - 1 : training on 467 raw words (310 effective words) took 0.0s, 96029 effective words/s\n",
      "2019-04-23 00:43:23,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:43:23,419 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:43:23,421 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:43:23,421 : INFO : EPOCH - 2 : training on 467 raw words (296 effective words) took 0.0s, 68783 effective words/s\n",
      "2019-04-23 00:43:23,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:43:23,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:43:23,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:43:23,428 : INFO : EPOCH - 3 : training on 467 raw words (289 effective words) took 0.0s, 82394 effective words/s\n",
      "2019-04-23 00:43:23,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:43:23,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:43:23,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:43:23,434 : INFO : EPOCH - 4 : training on 467 raw words (293 effective words) took 0.0s, 67347 effective words/s\n",
      "2019-04-23 00:43:23,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:43:23,442 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:43:23,443 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:43:23,444 : INFO : EPOCH - 5 : training on 467 raw words (300 effective words) took 0.0s, 57416 effective words/s\n",
      "2019-04-23 00:43:23,444 : INFO : training on a 2335 raw words (1488 effective words) took 0.0s, 41929 effective words/s\n",
      "2019-04-23 00:43:23,445 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 引入日志配置\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#这里使用了word2vec提供的LineSentence类来读文件\n",
    "sentences = word2vec.LineSentence('data_01.txt') \n",
    "\n",
    "# 创建模型\n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=3,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型出来了，我们可以用来做什么呢？这里给出三个常用的应用\n",
    "\n",
    "　　　**1.第一个是最常用的，找出某一个词向量最相近的词集合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:43:24.695230Z",
     "start_time": "2019-04-22T16:43:24.678861Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:43:24,680 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '你好' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-77c815a220d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreq_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m  \u001b[1;31m# 设置显示多少个匹配词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'你好'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 设置匹配词长度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mreq_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[1;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \"\"\"\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '你好' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "req_count = 5  # 设置显示多少个匹配词\n",
    "for key in model.wv.similar_by_word('你好', topn =100):\n",
    "    if len(key[0])>2:   # 设置匹配词长度\n",
    "        req_count -= 1\n",
    "        print (key[0], key[1])  #key\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man']) \n",
    "#输出[('queen', 0.50882536), ...] \n",
    "* * *\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split()) \n",
    "#输出'cereal' \n",
    "* * *\n",
    "model.similarity('woman', 'man') \n",
    "#输出0.73723527 \n",
    "* * *\n",
    "model['computer'] # raw numpy vector of a word \n",
    "#输出array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32) \n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:06:40.503491Z",
     "start_time": "2019-04-22T15:06:40.499027Z"
    }
   },
   "source": [
    "### word2vec模型训练\n",
    "\n",
    "这里我们调用Word2Vec创建模型实际上会对数据执行两次迭代操作，\n",
    "\n",
    "第一轮操作会统计词频来构建内部的词典数结构，\n",
    "\n",
    "第二轮操作会进行神经网络训练，而这两个步骤是可以分步进行的，这样对于某些不可重复的流（譬如 Kafka 等流式数据中）可以手动控制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:08:09.427199Z",
     "start_time": "2019-04-22T15:08:09.413309Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'some_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c9b5dd50eae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# an empty model, no training yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_sentences\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# can be a non-repeatable, 1-pass generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_sentences\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# can be a non-repeatable, 1-pass generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'some_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import gensimim\n",
    "model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "model.train(other_sentences)  # can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "### 处理批量文本\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Gensim中实现word2vec模型非常简单。首先，我们需要将原始的**训练语料转化成一个sentence的迭代器；每一次迭代返回的sentence是一个word的列表**\n",
    "\n",
    "#### 我们在上文中也提及，<font color=red>如果是对于大量的输入语料集或者需要整合磁盘上多个文件夹下的数据，我们可以以迭代器的方式而不是一次性将全部内容读取到内存中来节省 RAM 空间</font>："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:55:25.353999Z",
     "start_time": "2019-04-22T15:55:25.349039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1.txt\n",
      "data2.txt\n",
      "data3.txt\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir('D:\\\\test\\\\Gensim_构造词向量模板\\\\corpus'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:29:09.339323Z",
     "start_time": "2019-04-22T16:29:09.333372Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定类，生成迭代器\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                word_cut = jieba.cut(line)\n",
    "                yield word_cut\n",
    "\n",
    "sentences = MySentences('D:\\\\test\\\\Gensim_构造词向量模板\\\\corpus') # a memory-friendly iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:31:19.761164Z",
     "start_time": "2019-04-22T16:31:19.099086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:31:19,100 : INFO : collecting all words and their counts\n",
      "2019-04-23 00:31:19,104 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2019-04-23 00:31:19,106 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-23 00:31:19,448 : INFO : collected 12 word types from a corpus of 854 raw words and 61 sentences\n",
      "2019-04-23 00:31:19,449 : INFO : Loading a fresh vocabulary\n",
      "2019-04-23 00:31:19,450 : INFO : min_count=1 retains 12 unique words (100% of original 12, drops 0)\n",
      "2019-04-23 00:31:19,451 : INFO : min_count=1 leaves 854 word corpus (100% of original 854, drops 0)\n",
      "2019-04-23 00:31:19,452 : INFO : deleting the raw counts dictionary of 12 items\n",
      "2019-04-23 00:31:19,453 : INFO : sample=0.001 downsamples 12 most-common words\n",
      "2019-04-23 00:31:19,454 : INFO : downsampling leaves estimated 97 word corpus (11.5% of prior 854)\n",
      "2019-04-23 00:31:19,455 : INFO : estimated required memory for 12 words and 100 dimensions: 15600 bytes\n",
      "2019-04-23 00:31:19,456 : INFO : resetting layer weights\n",
      "2019-04-23 00:31:19,458 : INFO : training model with 3 workers on 12 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-23 00:31:19,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:31:19,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:31:19,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:31:19,522 : INFO : EPOCH - 1 : training on 854 raw words (87 effective words) took 0.1s, 1408 effective words/s\n",
      "2019-04-23 00:31:19,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:31:19,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:31:19,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:31:19,584 : INFO : EPOCH - 2 : training on 854 raw words (103 effective words) took 0.1s, 1733 effective words/s\n",
      "2019-04-23 00:31:19,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:31:19,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:31:19,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:31:19,640 : INFO : EPOCH - 3 : training on 854 raw words (119 effective words) took 0.1s, 2228 effective words/s\n",
      "2019-04-23 00:31:19,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:31:19,696 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:31:19,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:31:19,697 : INFO : EPOCH - 4 : training on 854 raw words (100 effective words) took 0.1s, 1870 effective words/s\n",
      "2019-04-23 00:31:19,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 00:31:19,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 00:31:19,756 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 00:31:19,756 : INFO : EPOCH - 5 : training on 854 raw words (103 effective words) took 0.1s, 1818 effective words/s\n",
      "2019-04-23 00:31:19,757 : INFO : training on a 4270 raw words (512 effective words) took 0.3s, 1718 effective words/s\n",
      "2019-04-23 00:31:19,757 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "#这里使用了word2vec提供的LineSentence类来读文件\n",
    "# sentence = word2vec.LineSentence(sentences) \n",
    "model = gensim.models.Word2Vec(sentences,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:32:59.683625Z",
     "start_time": "2019-04-22T16:32:59.655351Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '共商' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-87d91cb4bc3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 查看对应的词向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'共商'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                 )\n\u001b[1;32m-> 1398\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \"\"\"\n\u001b[1;32m--> 821\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '共商' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 查看对应的词向量\n",
    "model[u'共商'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，便完成了一个word2vec模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存与读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**文件保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:33:43.734291Z",
     "start_time": "2019-04-22T16:33:43.713502Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:33:43,714 : INFO : saving Word2Vec object under text2.model, separately None\n",
      "2019-04-23 00:33:43,714 : INFO : not storing attribute vectors_norm\n",
      "2019-04-23 00:33:43,715 : INFO : not storing attribute cum_table\n",
      "2019-04-23 00:33:43,730 : INFO : saved text2.model\n"
     ]
    }
   ],
   "source": [
    "model.save('text2.model')    # 保存在当前文件夹路径下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**文件读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:09:54.838193Z",
     "start_time": "2019-04-22T16:09:54.803474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:09:54,803 : INFO : loading Word2Vec object from text1.model\n",
      "2019-04-23 00:09:54,832 : INFO : loading wv recursively from text1.model.wv.* with mmap=None\n",
      "2019-04-23 00:09:54,833 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-04-23 00:09:54,833 : INFO : loading vocabulary recursively from text1.model.vocabulary.* with mmap=None\n",
      "2019-04-23 00:09:54,833 : INFO : loading trainables recursively from text1.model.trainables.* with mmap=None\n",
      "2019-04-23 00:09:54,834 : INFO : setting ignored attribute cum_table to None\n",
      "2019-04-23 00:09:54,834 : INFO : loaded text1.model\n"
     ]
    }
   ],
   "source": [
    "model1 = gensim.models.Word2Vec.load('text1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，word2vec对象也支持原始bin文件格式的读写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:11:23.876488Z",
     "start_time": "2019-04-22T16:11:23.871020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:11:23,872 : INFO : storing 32x100 projection weights into text.model.bin\n"
     ]
    }
   ],
   "source": [
    "# 写入二进制文件\n",
    "model.wv.save_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:12:05.220270Z",
     "start_time": "2019-04-22T16:12:05.110816Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:12:05,113 : INFO : loading projection weights from text.model.bin\n",
      "2019-04-23 00:12:05,216 : INFO : loaded (32, 100) matrix from text.model.bin\n"
     ]
    }
   ],
   "source": [
    "# 读取二进制文件\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### online learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们可以将更多的训练数据传递给一个已经训练好的word2vec对象，继续更新模型的参数：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:47:22.825701Z",
     "start_time": "2019-04-22T16:47:22.799907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:47:22,801 : INFO : loading Word2Vec object from text1.model\n",
      "2019-04-23 00:47:22,803 : INFO : loading wv recursively from text1.model.wv.* with mmap=None\n",
      "2019-04-23 00:47:22,804 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-04-23 00:47:22,804 : INFO : loading vocabulary recursively from text1.model.vocabulary.* with mmap=None\n",
      "2019-04-23 00:47:22,805 : INFO : loading trainables recursively from text1.model.trainables.* with mmap=None\n",
      "2019-04-23 00:47:22,805 : INFO : setting ignored attribute cum_table to None\n",
      "2019-04-23 00:47:22,806 : INFO : loaded text1.model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-b9000584ec72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msome_sentences\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             raise ValueError(\n\u001b[1;32m--> 614\u001b[1;33m                 \u001b[1;34m\"You must specify either total_examples or total_words, for proper job parameters updation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m                 \u001b[1;34m\"and progress calculations. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                 \u001b[1;34m\"The usual value is total_examples=model.corpus_count.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count."
     ]
    }
   ],
   "source": [
    "model3 = gensim.models.Word2Vec.load('text1.model')\n",
    "\n",
    "# model3.train(more_sentences)\n",
    "\n",
    "# 引入数据集\n",
    "raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\n",
    "some_sentences= [s.split() for s in raw_sentences]\n",
    "\n",
    "model3.train(some_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 外部语料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在真实的训练场景中我们往往会使用较大的语料集进行训练，譬如这里以 Word2Vec 官方的text8为例，只要改变模型中的语料集开源即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:50:53.830972Z",
     "start_time": "2019-04-22T16:50:53.746652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:50:53,748 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-2ecc6a3220b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m             self.train(\n\u001b[0;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \"\"\"\n\u001b[0;32m    479\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m--> 480\u001b[1;33m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[0;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;31m# so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# avoid loading the entire file (=1 line) into RAM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mode should be a string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text8'"
     ]
    }
   ],
   "source": [
    "# 载入官方语料集，进行训练（需要提前下载text8语料）\n",
    "sentences = word2vec.Text8Corpus('text8')\n",
    "model = word2vec.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 的训练属于无监督模型，并没有太多的类似于监督学习里面的客观评判方式，更多的依赖于端应用。Google 之前公开了20000条左右的语法与语义化训练样本，每一条遵循A is to B as C is to D这个格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy('/tmp/questions-words.txt')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
